{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../pyscrap_url.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_elements(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if '@' in name: #including conditional statement to filter data not needed\n",
    "                        if len(name) > 0:\n",
    "                            res.append(name.strip())\n",
    "                    else:\n",
    "                        res = res\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "                            \n",
    "        return res\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "    \n",
    "if get_ipython().__class__.__name__ == '__main__':\n",
    "    fire(get_tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 African Twitter Influencers: \n",
      "\n",
      "1. Trevor Noah (@Trevornoah)\n",
      "2. Gareth Cliff (@GarethCliff)\n",
      "3. Jacob G. Zuma (@SAPresident)\n",
      "4. News24 (@News24)\n",
      "5. Julius Sello Malema (@Julius_S_Malema)\n",
      "6. Helen Zille (@helenzille)\n",
      "7. mailandguardian (@mailandguardian)\n",
      "8. 5FM (@5FM)\n",
      "9. loyiso gola (@loyisogola)\n",
      "10. Computicket (@Computicket)\n"
     ]
    }
   ],
   "source": [
    "#python code to obtain top 10 African Twitter Influencers\n",
    "res = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa', tag='h2')\n",
    "res.reverse()\n",
    "\n",
    "#displaying the top 10 african twitter influencers\n",
    "print(\"Top 10 African Twitter Influencers: \\n\")\n",
    "for influencer in res[0:10]:\n",
    "    print(influencer)\n",
    "\n",
    "#saving data as csv file\n",
    "dat = pd.DataFrame(res[0:10], columns=['Infuencers'])\n",
    "dat.to_csv('top_10_african_influencers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findaing all of {'class_': 'wp-block-embed__wrapper'}\n",
      "\n",
      "10 African top government officials twitter handles: \n",
      "        \n",
      "------------------------------------------------------\n",
      "\n",
      "1. @EswatiniGovern1\n",
      "2. @MalawiGovt\n",
      "3. @hagegeingob\n",
      "4. @FinanceSC\n",
      "5. @PresidencyZA\n",
      "6. @Dora_Siliya\n",
      "7. @ChitaluChilufy3\n",
      "8. @noalaskinner\n",
      "9. @coumbagadio_ZM\n",
      "10. @unicefzambia\n"
     ]
    }
   ],
   "source": [
    "#Python code to obtain the twitter account of African top government officials\n",
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = simple_get(url)\n",
    "\n",
    "res_gov = get_elements(response, search={'find_all':{'class_':'wp-block-embed__wrapper'}})\n",
    "res_gov\n",
    "\n",
    "#converting the data to string\n",
    "list_gov = str(res_gov)\n",
    "\n",
    "#importing regular expression\n",
    "import re\n",
    "\n",
    "#deriving the government officials twitter handles using regex\n",
    "listnew = re.findall(r\"@[\\w]*\", list_gov)\n",
    "\n",
    "#displaying 10 african top government officials handles\n",
    "print(\"\"\"\\n10 African top government officials twitter handles: \n",
    "        \\n------------------------------------------------------\\n\"\"\")\n",
    "count = 1\n",
    "for hand in listnew[0:10]:\n",
    "    print(f\"{count}. {hand}\")\n",
    "    count = count+1\n",
    "\n",
    "#saving data as csv file\n",
    "dat2 = pd.DataFrame(listnew[0:10], columns=['Government Officials'])\n",
    "dat2.to_csv('10_government_officials.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
